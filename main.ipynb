{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import glob\n",
    "from openai import OpenAI, RateLimitError, APIError, APIConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read API key\n",
    "if not \"OPENAI_API_KEY\" in os.environ:\n",
    "    raise ValueError(\"could not find OPENAI_API_KEY in your path/environment variables\")\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API wrapper parameters\n",
    "model = \"gpt-5.1\"\n",
    "max_retries = 1\n",
    "initial_backoff = 1.0\n",
    "max_completion_tokens = 10000\n",
    "reasoning_effort = \"medium\" \n",
    "\n",
    "# Wrapper function w/ retries and exponential backoff\n",
    "def call_chat_completion(system_prompt: str, user_prompt: str, model: str = model,\n",
    "                          max_completion_tokens: int = max_completion_tokens,\n",
    "                          max_retries: int = max_retries, initial_backoff: float = initial_backoff,\n",
    "                          reasoning_effort: str = reasoning_effort) -> str:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI ChatCompletion endpoint with retries.\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                max_completion_tokens=max_completion_tokens,\n",
    "                reasoning_effort=reasoning_effort,\n",
    "            )\n",
    "            # check if max_completion_tokens is too low\n",
    "            finish_reason = getattr(response.choices[0], \"finish_reason\", None)\n",
    "            if finish_reason == \"length\":\n",
    "                print(f\"Warning: finish_reason='length'. Consider increasing max_completion_tokens.\")\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "\n",
    "        except (RateLimitError, APIError, APIConnectionError) as e:\n",
    "            print(f\"API error ({type(e).__name__}): retry {attempt}/{max_retries} after {initial_backoff:.1f}s...\")\n",
    "            time.sleep(initial_backoff)\n",
    "            initial_backoff *= 2  # Exponential backoff\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}. Retrying in {initial_backoff:.1f}s...\")\n",
    "            time.sleep(initial_backoff)\n",
    "            initial_backoff *= 2\n",
    "\n",
    "    raise RuntimeError(\"Failed to get response after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_path = \"prompts\\\\system_prompt.txt\"    \n",
    "if not os.path.exists(system_prompt_path):\n",
    "        print(f\"Prompt file '{system_prompt_path}' not found.\")\n",
    "\n",
    "\n",
    "with open(system_prompt_path, \"r\", encoding=\"utf-8\") as pf:\n",
    "    system_prompt = pf.read().strip()\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST RESPONSE: Run function and return output\n",
    "test_response = call_chat_completion(system_prompt = \"\", user_prompt=\"say test\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"inputs\" #set your input directory with the a .txt file for each question you want to generate exemplars for\n",
    "output_dir = \"outputs\" #set output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "prompts_dir = \"prompts\"\n",
    "system_prompt_path = \"prompts\\\\system_prompt.txt\"\n",
    "\n",
    "# read in system_prompt\n",
    "if not os.path.exists(system_prompt_path):\n",
    "        print(f\"Prompt file '{system_prompt_path}' not found.\")\n",
    "with open(system_prompt_path, \"r\", encoding=\"utf-8\") as pf:\n",
    "    system_prompt = pf.read().strip()\n",
    "\n",
    "def process_files(\n",
    "    performance_level: str,\n",
    "    input_dir: str = input_dir,\n",
    "    output_dir: str = output_dir,\n",
    "    prompts_dir: str = prompts_dir,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads .txt files from input_dir, uses a performance-level-specific prompt\n",
    "    from prompts_dir, sends content to GPT, and writes responses to output_dir.\n",
    "    \n",
    "    The prompt file in prompts_dir must be named like '<performance_level>.txt'\n",
    "    (e.g. 'poor.txt', 'excellent.txt'), and must contain exactly\n",
    "    two sections separated by a line of '---'.\n",
    "    The first section becomes the prefix, the second becomes the suffix.\n",
    "    \"\"\"\n",
    "    # Load performance-level-specific prompt\n",
    "    prompt_path = os.path.join(prompts_dir, f\"{performance_level}.txt\")\n",
    "    if not os.path.exists(prompt_path):\n",
    "        print(f\"Prompt file '{prompt_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as pf:\n",
    "        prompt_content = pf.read().strip()\n",
    "\n",
    "    if \"---\" in prompt_content:\n",
    "        parts = prompt_content.split(\"---\", 1)\n",
    "    else:\n",
    "        print(f\"Warning: No separator found in {prompt_path}. Using entire file as prefix.\")\n",
    "        parts = [prompt_content, \"\"]\n",
    "\n",
    "    per_file_prefix = parts[0].strip()\n",
    "    per_file_suffix = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "    print(f\"Using performance level: {performance_level}\")\n",
    "    print(f\"  Prefix length: {len(per_file_prefix)} chars\")\n",
    "    print(f\"  Suffix length: {len(per_file_suffix)} chars\")\n",
    "\n",
    "    # Process input files\n",
    "    txt_paths = sorted(glob.glob(os.path.join(input_dir, \"*.txt\")))\n",
    "    if not txt_paths:\n",
    "        print(f\"No .txt files found in {input_dir}.\")\n",
    "        return\n",
    "\n",
    "    for path in txt_paths:\n",
    "        filename = os.path.basename(path)\n",
    "        out_path = os.path.join(output_dir, f\"{performance_level}_{filename}\")\n",
    "        print(f\"\\nProcessing {filename} -> {out_path}\")\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        if not content:\n",
    "            print(\" Input file is empty â€” skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build the user prompt\n",
    "        user_prompt = f\"{per_file_prefix}{content}{per_file_suffix}\"\n",
    "\n",
    "        try:\n",
    "            output_text = call_chat_completion(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt\n",
    "                )\n",
    "            print(len(output_text))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "            output_text = f\"[ERROR] Could not get response: {e}\"\n",
    "\n",
    "        # Save output_text to output file\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as outf:\n",
    "            outf.write(output_text)\n",
    "\n",
    "        print(f\" Saved response ({len(output_text)} chars).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_files(performance_level=\"poor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_files(performance_level=\"sufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_files(performance_level=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_files(performance_level=\"excellent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
